{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Layers` in TensorFlow-Slim\n",
    "*by Marvin Bertin*\n",
    "<img src=\"../images/tensorflow.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow-Slim - `Layers`\n",
    "TF-Slim provides a set of  high level layers that makes building models in tensorflow simpler and more concise. <br>\n",
    "All layer related functions are accesssible in [layers.py](https://github.com/tensorflow/tensorflow/blob/eb56a8af24695bf8258addf28b0c53fbabff72e6/tensorflow/contrib/layers/python/layers/layers.py). <br>\n",
    "<br>\n",
    "<img src=\"../images/Conv2.png\" width=\"400\">\n",
    "<center>A 2D Convolutional Neural Network</center>\n",
    "\n",
    "The following methods will be covered in this notebook:\n",
    "```\n",
    "slim.conv2d\n",
    "slim.fully_connected\n",
    "slim.max_pool2d\n",
    "slim.dropout\n",
    "slim.softmax\n",
    "slim.repeat\n",
    "slim.stack\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.append(\"../\") \n",
    "\n",
    "from utils.pretty_printer import inspect_variables, inspect_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional and Maxpool Layers with Native TensorFlow\n",
    "Neural Network layers often consist of multiple variables and tensor operations combined together. For example a single convolutional layer takes in two variables( a weight tensor and bias vector) and combines them with a 2D convolution operation followed by a non-linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "<br>\n",
    "<div style=\"float:left;margin-right:5px;\">\n",
    "    <img src=\"../images/Conv3.jpeg\" width=\"300\" />\n",
    "    <p style=\"text-align:center;\">*2D Convolution on color image*</p>\n",
    "</div>\n",
    "<div style=\"float:center;margin-right:5px;\">\n",
    "    <img src=\"../images/neuron_model.jpeg\" width=\"350\" />\n",
    "    <p style=\"text-align:center;\">*A Neural Network \"neuron\"*</p>\n",
    "</div>\n",
    "\n",
    "<img src=\"../images/Convolution.gif\" width=\"400\">\n",
    "<center>Convolution with 3Ã—3 Filter</center>\n",
    "\n",
    "**Want to learn more?**\n",
    "- [Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/)\n",
    "- [Feature Extraction Using Convolution](http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/)\n",
    "- [Understanding Convolutions](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)\n",
    "- [Conv Nets: A Modular Perspective](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/)\n",
    "### Maxpool Layer\n",
    "<br>\n",
    "<div style=\"float:left;margin-right:5px;\">\n",
    "    <img src=\"../images/pool.jpeg\" width=\"300\" />\n",
    "    <p style=\"text-align:center;\">*Spatial downsampling with filter size 2, stride 2*</p>\n",
    "</div>\n",
    "<div style=\"float:center;margin-right:5px;\">\n",
    "    <img src=\"../images/maxpool.jpeg\" width=\"400\" />\n",
    "    <p style=\"text-align:center;\">*Maxpooling operation*</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Want to learn more?**\n",
    "- [Pooling: Overview](http://ufldl.stanford.edu/tutorial/supervised/Pooling/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native TensorFlow Code\n",
    "This creates a lot of boilerplate code that quickly becomes unreadabe and unmaintanable beyound a few layers. <br>\n",
    "Here is an example of a relativaly simple model combining 2 convolutional layers with a max-pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    # 4D Tensor placeholder for input images\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, 32, 32, 3], name=\"images\")\n",
    "\n",
    "    # Convolutional layers variable scope\n",
    "    with tf.variable_scope(\"TF-Native-conv\", [inputs]):\n",
    "        # conv1\n",
    "        with tf.variable_scope('conv1'):\n",
    "            weights = tf.get_variable('weights',\n",
    "                                      shape=[3, 3, 3, 64],\n",
    "                                      initializer=tf.random_normal_initializer(\n",
    "                                          stddev=0.1),\n",
    "                                      trainable=True,\n",
    "                                      dtype=tf.float32)\n",
    "            conv = tf.nn.conv2d(inputs, weights, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.get_variable('biases',\n",
    "                                     shape=[64],\n",
    "                                     initializer=tf.constant_initializer(0.0),\n",
    "                                     trainable=True,\n",
    "                                     dtype=tf.float32)\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            conv1 = tf.nn.relu(out, name=\"relu\")\n",
    "\n",
    "        # conv2\n",
    "        with tf.variable_scope('conv2'):\n",
    "            weights = tf.get_variable('weights',\n",
    "                                      shape=[3, 3, 64, 64],\n",
    "                                      initializer=tf.random_normal_initializer(\n",
    "                                          stddev=0.1),\n",
    "                                      trainable=True,\n",
    "                                      dtype=tf.float32)\n",
    "            conv = tf.nn.conv2d(conv1, weights, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.get_variable('biases',\n",
    "                                     shape=[64],\n",
    "                                     initializer=tf.constant_initializer(0.0),\n",
    "                                     trainable=True,\n",
    "                                     dtype=tf.float32)\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            conv2 = tf.nn.relu(out, name=\"relu\")\n",
    "\n",
    "        # pool3\n",
    "        with tf.variable_scope('pool3'):\n",
    "            outputs = tf.nn.max_pool(conv2,\n",
    "                                     ksize=[1, 2, 2, 1],\n",
    "                                     strides=[1, 2, 2, 1],\n",
    "                                     padding='SAME',\n",
    "                                     name='max_pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "name = TF-Native-conv/conv1/weights:0                          shape = (3, 3, 3, 64)\n",
      "name = TF-Native-conv/conv1/biases:0                           shape = (64,)\n",
      "name = TF-Native-conv/conv2/weights:0                          shape = (3, 3, 64, 64)\n",
      "name = TF-Native-conv/conv2/biases:0                           shape = (64,)\n",
      "\n",
      "Inputs/Outputs:\n",
      "name = images:0                                                shape = (?, 32, 32, 3)\n",
      "name = TF-Native-conv/pool3/max_pool:0                         shape = (?, 16, 16, 64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    print(\"Parameters:\")\n",
    "    inspect_variables(slim.get_variables(scope=\"TF-Native-conv\"))\n",
    "    print(\"Inputs/Outputs:\")\n",
    "    inspect_variables([inputs, outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional and Maxpool Layers with TensorFlow-Slim\n",
    "TF-Slim provides high level layer abstractions that removes boilerplate code while remaining flexible. <br>\n",
    "<br>\n",
    "For example `slim.conv2d` combines all these low level operations in one function:\n",
    "\n",
    "- Creates the weight and bias variables\n",
    "- Convolves the weights with the input from the previous layer\n",
    "- Adds the biases to the result of the convolution.\n",
    "- Applies an activation function.\n",
    "---\n",
    "    => slim.conv2d(*args, **kwargs)\n",
    "    Docstring:\n",
    "    Adds a 2D convolution followed by an optional batch_norm layer.\n",
    "\n",
    "    Args:\n",
    "      inputs: a 4-D tensor  `[batch_size, height, width, channels]`.\n",
    "      num_outputs: integer, the number of output filters.\n",
    "      kernel_size: a list of length 2 `[kernel_height, kernel_width]` of\n",
    "        of the filters.\n",
    "      stride: a list of length 2 `[stride_height, stride_width]`.\n",
    "      padding: one of `VALID` or `SAME`.\n",
    "      activation_fn: activation function, set to None to skip it and maintain\n",
    "        a linear activation.\n",
    "      normalizer_fn: normalization function to use instead of `biases`. If\n",
    "        `normalizer_fn` is provided then `biases_initializer` and\n",
    "        `biases_regularizer` are ignored and `biases` are not created nor added.\n",
    "        default set to None for no normalizer function\n",
    "      normalizer_params: normalization function parameters.\n",
    "      weights_initializer: An initializer for the weights.\n",
    "      weights_regularizer: Optional regularizer for the weights.\n",
    "      biases_initializer: An initializer for the biases. If None skip biases.\n",
    "      biases_regularizer: Optional regularizer for the biases.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      ...\n",
    "\n",
    "    Returns:\n",
    "      a tensor representing the output of the operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code with TF-Slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    # 4D Tensor placeholder for input images\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, 32, 32, 3], name=\"images\")\n",
    "\n",
    "    with tf.variable_scope(\"TF-Slim-conv\", [inputs]):\n",
    "        # conv1\n",
    "        net = slim.conv2d(inputs, 64, [3, 3],\n",
    "                          padding='SAME',\n",
    "                          weights_initializer = tf.random_normal_initializer(stddev=0.1),\n",
    "                          scope='conv1')\n",
    "        # conv2\n",
    "        net = slim.conv2d(net, 64, [3, 3],\n",
    "                          padding='SAME',\n",
    "                          weights_initializer = tf.random_normal_initializer(stddev=0.1),\n",
    "                          scope='conv2')\n",
    "        # pool3\n",
    "        outputs = slim.max_pool2d(net, [2, 2], scope = \"pool3\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "name = TF-Slim-conv/conv1/weights:0                            shape = (3, 3, 3, 64)\n",
      "name = TF-Slim-conv/conv1/biases:0                             shape = (64,)\n",
      "name = TF-Slim-conv/conv2/weights:0                            shape = (3, 3, 64, 64)\n",
      "name = TF-Slim-conv/conv2/biases:0                             shape = (64,)\n",
      "\n",
      "Inputs/Outputs:\n",
      "name = images:0                                                shape = (?, 32, 32, 3)\n",
      "name = TF-Slim-conv/pool3/MaxPool:0                            shape = (?, 16, 16, 64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    print(\"Parameters:\")\n",
    "    inspect_variables(slim.get_variables(scope=\"TF-Slim-conv\"))\n",
    "    print(\"Inputs/Outputs:\")\n",
    "    inspect_variables([inputs, outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-operation `slim.repeat`\n",
    "TF-Slim also provides two meta-operations called `slim.repeat` and `slim.stack` that allow to repeat the same operation multiple times and reduce the code size even further.\n",
    "\n",
    "    => slim.repeat(inputs, repetitions, layer, *args, **kwargs)\n",
    "    Docstring:\n",
    "    Applies the same layer with the same arguments repeatedly.\n",
    "\n",
    "    Args:\n",
    "      inputs: A `Tensor` suitable for layer.\n",
    "      repetitions: Int, number of repetitions.\n",
    "      layer: A layer with arguments `(inputs, *args, **kwargs)`\n",
    "      *args: Extra args for the layer.\n",
    "      **kwargs: Extra kwargs for the layer.\n",
    "\n",
    "    Returns:\n",
    "      a tensor result of applying the layer, repetitions times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, 32, 32, 3], name=\"images\")\n",
    "\n",
    "    with tf.variable_scope(\"TF-Slimer-conv\", [inputs]):\n",
    "        # conv 1-2\n",
    "        net = slim.repeat(inputs, 2,\n",
    "                          slim.conv2d,\n",
    "                          64, [3, 3],\n",
    "                          padding='SAME',\n",
    "                          weights_initializer = tf.random_normal_initializer(stddev=0.1),\n",
    "                          scope='conv')\n",
    "        # pool\n",
    "        outputs = slim.max_pool2d(net, [2, 2], scope='pool3')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "name = TF-Slimer-conv/conv/conv_1/weights:0                    shape = (3, 3, 3, 64)\n",
      "name = TF-Slimer-conv/conv/conv_1/biases:0                     shape = (64,)\n",
      "name = TF-Slimer-conv/conv/conv_2/weights:0                    shape = (3, 3, 64, 64)\n",
      "name = TF-Slimer-conv/conv/conv_2/biases:0                     shape = (64,)\n",
      "\n",
      "Inputs/Outputs:\n",
      "name = images:0                                                shape = (?, 32, 32, 3)\n",
      "name = TF-Slimer-conv/pool3/MaxPool:0                          shape = (?, 16, 16, 64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    print(\"Parameters:\")\n",
    "    inspect_variables(slim.get_variables(scope=\"TF-Slimer-conv\"))\n",
    "    print(\"Inputs/Outputs:\")\n",
    "    inspect_variables([inputs, outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layers in Naive TensorFlow\n",
    "A feedforward (fully connected) layer is the most basic layer in neural networks.\n",
    "<div style=\"float:right;margin-right:5px;\">\n",
    "    <img src=\"../images/SingleNeuron.png\" width=\"300\" />\n",
    "    <p style=\"text-align:center;\">*Single feedforward neuron*</p>\n",
    "</div>\n",
    "<br>\n",
    "**Feedforward computation**\n",
    "\n",
    "$\\textstyle h_{W,b}(x) = f(W^Tx) = f(\\sum_{i=1}^3 W_{i}x_i +b)$ <br>\n",
    "\n",
    "$f =$ activation function <br>\n",
    "$W =$ weight vector/matrix <br>\n",
    "$b =$ bias scalar/vector <br>\n",
    "\n",
    "**Want to learn more?**\n",
    "- [Multi-Layer Neural Network](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)\n",
    "- [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU) (video series)\n",
    "- [Neural Network in Python](https://github.com/MarvinBertin/NNet/blob/master/Neural%20Net.ipynb) (from scratch)\n",
    "### Multi-layer Neural Network\n",
    "<br>\n",
    "<img src=\"../images/fc.jpeg\" width=\"500\">\n",
    "### Native TensorFlow Code\n",
    "Although this neural network is very simple, the tensorflow code is quite lengthy and repetitive. The neural network ends with a dropout layer and a softmax layer.\n",
    "You can learn more about these layers below.\n",
    "<img src=\"../images/dropout1.png\" width=\"600\">\n",
    "\n",
    "**Want to learn more?**\n",
    "- [Deep learning - Dropout](https://www.youtube.com/watch?v=UcKPdAM8cnI)\n",
    "- [Softmax Regression](http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/)\n",
    "- [Linear Classification](http://cs231n.github.io/linear-classify/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # 2D tensor placeholder for input data (512 features)\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, 512], name=\"inputs\")\n",
    "    \n",
    "    with tf.variable_scope('TF-Native-fc', [inputs]):\n",
    "\n",
    "        # fc1\n",
    "        with tf.variable_scope('fc1'):\n",
    "            # weight matrix\n",
    "            fcw = tf.get_variable('weights',\n",
    "                                  shape=[512, 1024],\n",
    "                                  initializer=tf.random_normal_initializer(stddev=0.1),\n",
    "                                  trainable=True,\n",
    "                                  dtype=tf.float32) \n",
    "            # bias vector\n",
    "            fcb = tf.get_variable('biases',\n",
    "                                  shape=[1024],\n",
    "                                  initializer=tf.constant_initializer(1.0),\n",
    "                                  trainable=True,\n",
    "                                  dtype=tf.float32)\n",
    "            # dot product with bias term\n",
    "            fcl = tf.nn.bias_add(tf.matmul(inputs, fcw), fcb)\n",
    "            # activation function\n",
    "            fc = tf.nn.relu(fcl)\n",
    "        \n",
    "        # fc2\n",
    "        with tf.variable_scope('fc2'):\n",
    "            fcw = tf.get_variable('weights',\n",
    "                                  shape=[1024, 256],\n",
    "                                  initializer=tf.random_normal_initializer(stddev=0.1),\n",
    "                                  trainable=True,\n",
    "                                  dtype=tf.float32) \n",
    "\n",
    "            fcb = tf.get_variable('biases',\n",
    "                                 shape=[256],\n",
    "                                 initializer=tf.constant_initializer(1.0),\n",
    "                                 trainable=True,\n",
    "                                 dtype=tf.float32)\n",
    "            fcl = tf.nn.bias_add(tf.matmul(fc, fcw), fcb)\n",
    "            fc = tf.nn.relu(fcl)\n",
    "        \n",
    "        # fc3\n",
    "        with tf.variable_scope('fc3'):\n",
    "            fcw = tf.get_variable('weights',\n",
    "                                  shape=[256, 10],\n",
    "                                  initializer=tf.random_normal_initializer(stddev=0.1),\n",
    "                                  trainable=True,\n",
    "                                  dtype=tf.float32) \n",
    "\n",
    "            fcb = tf.get_variable('biases',\n",
    "                                 shape=[10],\n",
    "                                 initializer=tf.constant_initializer(1.0),\n",
    "                                 trainable=True,\n",
    "                                 dtype=tf.float32)\n",
    "            fcl = tf.nn.bias_add(tf.matmul(fc, fcw), fcb)\n",
    "            fc = tf.nn.relu(fcl)\n",
    "        \n",
    "        # dropout\n",
    "        with tf.variable_scope('dropout4'):\n",
    "            fc = tf.nn.dropout(fc, 0.5, name='dropout')\n",
    "        \n",
    "        # softmax\n",
    "        with tf.variable_scope('softmax5'):\n",
    "            outputs = tf.nn.softmax(fc, name='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "name = TF-Native-fc/fc1/weights:0                              shape = (512, 1024)\n",
      "name = TF-Native-fc/fc1/biases:0                               shape = (1024,)\n",
      "name = TF-Native-fc/fc2/weights:0                              shape = (1024, 256)\n",
      "name = TF-Native-fc/fc2/biases:0                               shape = (256,)\n",
      "name = TF-Native-fc/fc3/weights:0                              shape = (256, 10)\n",
      "name = TF-Native-fc/fc3/biases:0                               shape = (10,)\n",
      "\n",
      "Inputs/Outputs:\n",
      "name = inputs:0                                                shape = (?, 512)\n",
      "name = TF-Native-fc/softmax5/softmax:0                         shape = (?, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    print(\"Parameters:\")\n",
    "    inspect_variables(slim.get_variables(scope=\"TF-Native-fc\"))\n",
    "    print(\"Inputs/Outputs:\")\n",
    "    inspect_variables([inputs, outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layers in TensorFlow-Slim\n",
    "TF-slim provides you with a fully connected layer implementation that abstracts the layer very similarly to the convolutional layer we worked with earlier.\n",
    "\n",
    "    => slim.fully_connected(*args, **kwargs)\n",
    "    Docstring:\n",
    "    Adds a fully connected layer.\n",
    "\n",
    "    Args:\n",
    "      inputs: A tensor of with at least rank 2 and value for the last dimension,\n",
    "        i.e. `[batch_size, depth]`.\n",
    "      num_outputs: Integer or long, the number of output units in the layer.\n",
    "      activation_fn: activation function, set to None to skip it and maintain\n",
    "        a linear activation.\n",
    "      normalizer_fn: normalization function to use instead of `biases`. If\n",
    "        `normalizer_fn` is provided then `biases_initializer` and\n",
    "        `biases_regularizer` are ignored and `biases` are not created nor added.\n",
    "        default set to None for no normalizer function\n",
    "      normalizer_params: normalization function parameters.\n",
    "      weights_initializer: An initializer for the weights.\n",
    "      weights_regularizer: Optional regularizer for the weights.\n",
    "      biases_initializer: An initializer for the biases. If None skip biases.\n",
    "      biases_regularizer: Optional regularizer for the biases.\n",
    "      scope: Optional scope for variable_scope.\n",
    "      ...\n",
    "\n",
    "    Returns:\n",
    "       the tensor variable representing the result of the series of operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code with TF-Slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, 512], name=\"inputs\")\n",
    "\n",
    "    with tf.variable_scope('TF-Slim-fc', [inputs]):\n",
    "        # fc1\n",
    "        net = slim.fully_connected(inputs, 1024,\n",
    "                                   weights_initializer = tf.random_normal_initializer(\n",
    "                                    stddev=0.1),\n",
    "                                   scope='fc1')\n",
    "        # fc2\n",
    "        net = slim.fully_connected(net, 256,\n",
    "                                   weights_initializer = tf.random_normal_initializer(\n",
    "                                    stddev=0.1),\n",
    "                                   scope='fc2')\n",
    "        # fc3\n",
    "        net = slim.fully_connected(net, 10,\n",
    "                                   weights_initializer = tf.random_normal_initializer(\n",
    "                                    stddev=0.1),\n",
    "                                   scope='fc3')\n",
    "        # dropout\n",
    "        net = slim.dropout(net, 0.5, scope='dropout4')\n",
    "        # softmax\n",
    "        outputs = slim.softmax(net, scope='softmax5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "name = TF-Slim-fc/fc1/weights:0                                shape = (512, 1024)\n",
      "name = TF-Slim-fc/fc1/biases:0                                 shape = (1024,)\n",
      "name = TF-Slim-fc/fc2/weights:0                                shape = (1024, 256)\n",
      "name = TF-Slim-fc/fc2/biases:0                                 shape = (256,)\n",
      "name = TF-Slim-fc/fc3/weights:0                                shape = (256, 10)\n",
      "name = TF-Slim-fc/fc3/biases:0                                 shape = (10,)\n",
      "\n",
      "Inputs/Outputs:\n",
      "name = inputs:0                                                shape = (?, 512)\n",
      "name = TF-Slim-fc/softmax5/Reshape_1:0                         shape = (?, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    print(\"Parameters:\")\n",
    "    inspect_variables(slim.get_variables(scope=\"TF-Slim-fc\"))\n",
    "    print(\"Inputs/Outputs:\")\n",
    "    inspect_variables([inputs, outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-operation `slim.stack`\n",
    "\n",
    "    => slim.stack(inputs, layer, stack_args, **kwargs)\n",
    "    Docstring:\n",
    "    Builds a stack of layers by applying layer repeatedly using stack_args.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A `Tensor` suitable for layer.\n",
    "      layer: A layer with arguments `(inputs, *args, **kwargs)`\n",
    "      stack_args: A list/tuple of parameters for each call of layer.\n",
    "      **kwargs: Extra kwargs for the layer.\n",
    "\n",
    "    Returns:\n",
    "      a `Tensor` result of applying the stacked layers.\n",
    "      \n",
    "### Code with TF-Slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, 512], name=\"inputs\")\n",
    "\n",
    "    with tf.variable_scope('TF-Slim-fc', [inputs]):\n",
    "        # fc1-3\n",
    "        net = slim.stack(inputs, slim.fully_connected, [1024, 256, 10],\n",
    "                         weights_initializer = tf.random_normal_initializer(stddev=0.1),\n",
    "                         scope='fc')\n",
    "        # dropout\n",
    "        net = slim.dropout(net, 0.5, scope='dropout4')\n",
    "        # softmax\n",
    "        outputs = slim.softmax(net, scope='softmax5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "name = TF-Slim-fc/fc/fc_1/weights:0                            shape = (512, 1024)\n",
      "name = TF-Slim-fc/fc/fc_1/biases:0                             shape = (1024,)\n",
      "name = TF-Slim-fc/fc/fc_2/weights:0                            shape = (1024, 256)\n",
      "name = TF-Slim-fc/fc/fc_2/biases:0                             shape = (256,)\n",
      "name = TF-Slim-fc/fc/fc_3/weights:0                            shape = (256, 10)\n",
      "name = TF-Slim-fc/fc/fc_3/biases:0                             shape = (10,)\n",
      "\n",
      "Inputs/Outputs:\n",
      "name = inputs:0                                                shape = (?, 512)\n",
      "name = TF-Slim-fc/softmax5/Reshape_1:0                         shape = (?, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    print(\"Parameters:\")\n",
    "    inspect_variables(slim.get_variables(scope=\"TF-Slim-fc\"))\n",
    "    print(\"Inputs/Outputs:\")\n",
    "    inspect_variables([inputs, outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "### `Scope` in TensorFlow-Slim\n",
    "-  Define default arguments for specific operations within a scope\n",
    "\n",
    "<img src=\"../images/divider.png\" width=\"100\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:TF-latest]",
   "language": "python",
   "name": "conda-env-TF-latest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
