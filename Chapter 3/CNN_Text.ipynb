{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Convolutional Neural Network with TF-Slim\n",
    "*by Marvin Bertin*\n",
    "<img src=\"../images/tensorflow.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network** are typically used in Computer Vision. CNNs have been responsible for major breakthroughs in Image Classification tasks. However CNNs can also be used to solve problems in **Natural Language Processing**.\n",
    "\n",
    "**Text data**\n",
    "\n",
    "Text data, as opposed to static images, is sequential in nature and therefore have temporal dependencies compared to spacial dependencies in images.\n",
    "\n",
    "** Reccurent Neural Networks**\n",
    "\n",
    "Traditionally, text data is naturally suited for **Reccurent Neural Networks (RNNs)** where connections between units form a directed cycle. This creates an **internal state** of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process **arbitrary sequences of inputs**.\n",
    "\n",
    "<img src=\"../images/RNN.png\" width=\"600\">\n",
    "\n",
    "The problem with RNNs is that they are computationally heavy compared to CNNs and therefore slow to train on high dimensional data like text. On the other hand, CNNs are fast and highly optimized on GPUs.\n",
    "\n",
    "## Redefining Sequential Data to Fit CNNs\n",
    "In this notebook, we will see how we can apply tricks to our text classfication tasks, and redefine it in the context of an image, while still performing well a classification task.\n",
    "\n",
    "**Image vector representation**\n",
    "\n",
    "- each pixel is a different feature/dimension\n",
    "- pixel are representated as floating point numbers centered at zero\n",
    "- 3D tensor (vertical number of pixel) x (horizontal number of pixel) x (number of channel - 3 for colored images)\n",
    "\n",
    "**Text vector representation**\n",
    "\n",
    "- text is tokenized at the word or character level\n",
    "- tokens are represented as vector embeddings of floating point numbers (or one-hot vectors)\n",
    "- 3D tensor (text sequence length) x (token embedding size) x (1 dimension - similar to black and white images)\n",
    "\n",
    "**Image 2D convolutions**\n",
    "\n",
    "- filters (feature maps) slide both horizontally and vertically over the image\n",
    "\n",
    "**Text 1D convolutions**\n",
    "\n",
    "- filters (feature maps) only slide vertically along the text sequence dimension.\n",
    "- filters always have an horizontal dimension equal to the token embedding length.\n",
    "\n",
    "\n",
    "## Text CNN Model\n",
    "<img src=\"../images/text-cnn.png\" width=\"400\">\n",
    "\n",
    "** Want to learn more**\n",
    "- [understanding convolutional neural networks for nlp](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n",
    "- [Deep Learning, NLP, and Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n",
    "- [Visualizing Representations: Deep Learning and Human Beings](http://colah.github.io/posts/2015-01-Visualizing-Representations/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Tensorflow Slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.append(\"../\") \n",
    "\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Text CNN Model\n",
    "```\n",
    "embedding_size = 20\n",
    "num_filters = 10\n",
    "seq_length = 100\n",
    "\n",
    "with slim.arg_scope([slim.conv2d, slim.max_pool2d]):\n",
    "    branches = []\n",
    "    for i, filter_size in enumerate([3,4,5]):\n",
    "        with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "            # conv 1\n",
    "            net = slim.conv2d(inputs, num_filters, [filter_size, embedding_size],\n",
    "                              stride = [1,1], padding=\"VALID\",scope='1D-conv_%d'%(i+1))\n",
    "            # max-pool 2\n",
    "            net = slim.max_pool2d(net, [seq_length-filter_size+1, 1], stride = [1,1],\n",
    "                                  scope=\"1D-pool-%d\" % (i+1))\n",
    "            # append branch to stack\n",
    "            branches.append(net)\n",
    "    \n",
    "    # concatenate 3 branches\n",
    "    net = tf.concat(3, branches)\n",
    "    \n",
    "    # dropout\n",
    "    net = slim.dropout(net, 0.5, is_training=is_training, scope='dropout4')\n",
    "\n",
    "    # fully connected layer\n",
    "    net = slim.conv2d(net, self.output_dim, [1, 1],\n",
    "                      activation_fn=None,\n",
    "                      normalizer_fn=None,\n",
    "                      scope='prediction')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Text CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers\n",
      "name = CNN_dbpedia_text_classifier/conv-maxpool-3/1D-conv_1/Relu:0 shape = (?, 98, 1, 10)\n",
      "name = CNN_dbpedia_text_classifier/conv-maxpool-3/1D-pool-1/MaxPool:0 shape = (?, 1, 1, 10)\n",
      "name = CNN_dbpedia_text_classifier/conv-maxpool-4/1D-conv_2/Relu:0 shape = (?, 97, 1, 10)\n",
      "name = CNN_dbpedia_text_classifier/conv-maxpool-4/1D-pool-2/MaxPool:0 shape = (?, 1, 1, 10)\n",
      "name = CNN_dbpedia_text_classifier/conv-maxpool-5/1D-conv_3/Relu:0 shape = (?, 96, 1, 10)\n",
      "name = CNN_dbpedia_text_classifier/conv-maxpool-5/1D-pool-3/MaxPool:0 shape = (?, 1, 1, 10)\n",
      "name = CNN_dbpedia_text_classifier/prediction/squeezed:0       shape = (?, 15)\n",
      "\n",
      "\n",
      "Parameters\n",
      "name = CNN_dbpedia_text_classifier/1D-conv_1/weights:0         shape = (3, 20, 1, 10)\n",
      "name = CNN_dbpedia_text_classifier/1D-conv_1/biases:0          shape = (10,)\n",
      "name = CNN_dbpedia_text_classifier/1D-conv_2/weights:0         shape = (4, 20, 1, 10)\n",
      "name = CNN_dbpedia_text_classifier/1D-conv_2/biases:0          shape = (10,)\n",
      "name = CNN_dbpedia_text_classifier/1D-conv_3/weights:0         shape = (5, 20, 1, 10)\n",
      "name = CNN_dbpedia_text_classifier/1D-conv_3/biases:0          shape = (10,)\n",
      "name = CNN_dbpedia_text_classifier/prediction/weights:0        shape = (1, 1, 30, 15)\n",
      "name = CNN_dbpedia_text_classifier/prediction/biases:0         shape = (15,)\n"
     ]
    }
   ],
   "source": [
    "from utils.slim_models import CNNClassifier\n",
    "# [in_height, in_width, in_channels] (seq_length, embedding_size, channel)\n",
    "text_tensor_shape = (100, 20, 1)\n",
    "num_class = 15\n",
    "\n",
    "CNN_model = CNNClassifier(\"dbpedia\", text_tensor_shape , num_class)\n",
    "CNN_model.examine_model_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Text Data\n",
    "\n",
    "Text data cannot be fed directly into the neural network.\n",
    "A number of preprocessing steps need to happend first.\n",
    "\n",
    "- Tokenize the text into words and remove words with very low frequency\n",
    "- Transform the training and test data into sequences of token ids\n",
    "- Truncate every sequence to a fixed set length, or pad with zeros shorter sequences\n",
    "- Generate a trainable and regularized vector embedding for each tokken in your vocabulary\n",
    "- Load the sample sequences in batches by using an embedding lookup matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_vocabulary(x_train, x_test, max_document_length = 100):\n",
    "\n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "        max_document_length, min_frequency=2)\n",
    "    x_train = np.array(list(vocab_processor.fit_transform(x_train)))\n",
    "    x_test = np.array(list(vocab_processor.transform(x_test)))\n",
    "    n_words = len(vocab_processor.vocabulary_)\n",
    "    print('Total words: %d' % n_words)\n",
    "    return x_train, x_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Tansform Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.datasets import text_datasets\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def get_dbpedia_dataset(size='small'):\n",
    "    data = text_datasets.load_dbpedia(size=size)\n",
    "    \n",
    "    x_train = pd.DataFrame(data.train.data)[1]\n",
    "    y_train = pd.Series(data.train.target)\n",
    "    x_test = pd.DataFrame(data.test.data)[1]\n",
    "    y_test = pd.Series(data.test.target)\n",
    "    \n",
    "    x_train, x_test = preprocess_vocabulary(x_train, x_test, max_document_length = 100)\n",
    "    \n",
    "    dataset = defaultdict(dict)\n",
    "    dataset['train']['X'] = x_train\n",
    "    dataset['train']['y'] = y_train\n",
    "    dataset['test']['X'] = x_test\n",
    "    dataset['test']['y'] = y_test\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Embedding Lookup Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_inputs(data, data_type, n_words, weight_decay=0.005):\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(\n",
    "        data[data_type]['X'], vocab_size=n_words, embed_dim=20,\n",
    "        initializer=tf.truncated_normal_initializer(),\n",
    "        regularizer=slim.l2_regularizer(weight_decay),\n",
    "        trainable=True)\n",
    "\n",
    "    batch_indices = np.random.choice(n_words,32,replace=False)\n",
    "    inputs = tf.nn.embedding_lookup(word_vectors, batch_indices)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 1457\n"
     ]
    }
   ],
   "source": [
    "# load and transform data\n",
    "dataset = get_dbpedia_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0, 1044,    4,    6,   66,   37,    0, 1280, 1321,    5,\n",
       "       1265, 1321,  941,  188, 1280,    5, 1265,    0,    0,  220,    0,\n",
       "         42,  921,  298,    9,  852,    5,    0,   14,    0,    5,    0,\n",
       "          0,    0,    0,    8,  226,    2,    0,    2,  278,    0,    0,\n",
       "         12,    4,    1,  190,   81,   64,  386,    5,  909, 1354,    3,\n",
       "       1265,    5, 1280, 1321,    0,  127,   22,   32,    0,  908,    2,\n",
       "        155,  304,  359,  310,  607,    5,    1,  832,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text representation in terms of token ids\n",
    "dataset['train']['X'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Sample Batch Input for Text CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(32), Dimension(100), Dimension(20)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = get_batch_inputs(dataset, 'train', 1457)\n",
    "\n",
    "# Shape of input tensor into text CNN\n",
    "# (batch size, sequence length, word embedding size)\n",
    "inputs.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/divider.png\" width=\"100\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:TF-latest]",
   "language": "python",
   "name": "conda-env-TF-latest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
